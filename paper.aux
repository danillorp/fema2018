\relax 
\citation{CortesML:95}
\citation{HungNeuro:93}
\citation{AdeliMLN:94}
\citation{LinTSMCS:15a}
\citation{LinTSMCS:15b}
\citation{LiuTSMCS:16}
\citation{AdeliAMC:94}
\citation{AdeliJSA:93}
\citation{ChouCACIE:15}
\citation{LeCunNature:15}
\citation{RafieiCEM:16}
\citation{PapaISVC:08}
\citation{PapaIJIST:09}
\citation{PapaPR:12}
\citation{PapaPRL:17}
\citation{PapaIJIST:09}
\citation{PapaPR:12}
\citation{RumelhartNature:86}
\citation{HaganIEEETNN:94}
\citation{LiuIEEETSMC:11}
\citation{Lin:15}
\citation{Martinel:15}
\citation{Specht:90}
\citation{AhmadlouICAE:10}
\citation{Specht:90}
\citation{AdeliNN:09}
\citation{SankariJNM:11}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\newlabel{s.introduction}{{I}{1}}
\citation{AhmadlouICAE:10}
\citation{Sankari:11}
\citation{Hirschauer:15}
\citation{Zienkiewicz:67}
\citation{YuJSE:93}
\@writefile{toc}{\contentsline {section}{\numberline {II}Finite Element Method}{2}}
\newlabel{s.fem}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Function Approximation}{2}}
\newlabel{ss.function}{{\unhbox \voidb@x \hbox {II-A}}{2}}
\newlabel{sss.basis}{{\unhbox \voidb@x \hbox {II-A}1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}1}Approximation Basis}{2}}
\newlabel{sss.interpolation}{{\unhbox \voidb@x \hbox {II-A}2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}2}Interpolation}{2}}
\citation{Lehtinen:08}
\citation{Pereira:10}
\citation{Shepard:68}
\citation{Samworth:12}
\newlabel{sss.interpolating_bases}{{\unhbox \voidb@x \hbox {II-A}3}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-A}3}Interpolating Bases}{3}}
\newlabel{eq.condInterpolating}{{6}{3}}
\newlabel{e.interpolating_basis}{{7}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Partition of Unity Basis}{3}}
\newlabel{ss.partition}{{\unhbox \voidb@x \hbox {II-B}}{3}}
\newlabel{e.partition_unity1}{{8}{3}}
\newlabel{e.partition_unity2}{{9}{3}}
\newlabel{e.normalization}{{11}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-C}}Finite Element Basis}{3}}
\newlabel{ss.basis}{{\unhbox \voidb@x \hbox {II-C}}{3}}
\newlabel{sss.shepard}{{\unhbox \voidb@x \hbox {II-C}1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {II-C}1}Shepard Basis}{3}}
\newlabel{eq.shepard_basis}{{12}{3}}
\newlabel{eq.w}{{13}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Behaviour of different Shepard bases according to three values of $k$, where the black dots stand for the center of the basis: (a) $k=1$, (b) $k=3$ and (c) $k = 5$.}}{3}}
\newlabel{fig.elemShepard}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Interpolated function using the Shepard basis for (a) $k=1$, (b) $k=3$ and (c) $k=5$. The blue rectangles represent the center of the basis and their sampled values.}}{4}}
\newlabel{fig.intShepard}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Finite Element Machine}{4}}
\newlabel{s.fema}{{III}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Background Theory}{4}}
\newlabel{ss.background}{{\unhbox \voidb@x \hbox {III-A}}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Probabilistic Manifold Learning}{4}}
\newlabel{ss.manifold}{{\unhbox \voidb@x \hbox {III-B}}{4}}
\newlabel{eq.certainty}{{17}{4}}
\citation{Coppersmith:90}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The Shepard approximation of the probability function of a two-class problem using $k=3$ considering a given sample $\textbf  {x}$: (a) $P_1(\textbf  {x})$ and (b) $P_2(\textbf  {x})$. The red dots and the red curve denote the samples and the probability function of class $1$, respectively, and the black dots and the black curve stand for the samples and the probability function of class $2$, respectively. In (c), we have the two probability functions together. Notice each real number in $[-3,3]$ (i.e. $x$-axis) is classified according to the class that has the higher probability value (i.e. $y$-axis).}}{5}}
\newlabel{fig.ProbFunc}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Toy Example}{5}}
\newlabel{ss.toy}{{\unhbox \voidb@x \hbox {III-C}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces FEMa working mechanism: (a) training set with samples distributed in three classes, and the image classified by FEMa using (b) $k=1$, (c) $k=3$ and (d) $k=5$.}}{5}}
\newlabel{2Dpoints}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Probability map (degree of certainty) computed by FEMa in (a), and the test samples with their class label weighted by their respective degree of certainty.}}{5}}
\newlabel{fig.2Dcertain}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Complexity Analysis}{5}}
\newlabel{ss.complexity}{{\unhbox \voidb@x \hbox {III-D}}{5}}
\citation{LibOPF:14}
\citation{Wilcoxon:45}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments}{6}}
\newlabel{s.methodology}{{IV}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Information about the small datasets used in the experiments.}}{6}}
\newlabel{tab.datasets1}{{I}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Information about the medium-to-large datasets used in the experiments.}}{6}}
\newlabel{tab.datasets2}{{II}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Small-sized Datasets}{7}}
\newlabel{ss.small}{{\unhbox \voidb@x \hbox {IV-A}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Medium-to-large-sized Datasets}{7}}
\newlabel{ss.medium_to_large}{{\unhbox \voidb@x \hbox {IV-B}}{7}}
\newlabel{RF1}{7}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Accuracy using 50\% of the samples for training with normalized features.}}{7}}
\newlabel{tab.ACC_1}{{III}{7}}
\newlabel{RF2}{8}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Accuracy using 50\% of the samples for training with non-normalized features.}}{8}}
\newlabel{tab.ACC_0}{{IV}{8}}
\newlabel{RF3}{8}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Mean training time using 50\% of the samples for training without normalized features.}}{8}}
\newlabel{tab.TIME_0_TR}{{V}{8}}
\newlabel{RF4}{9}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Mean training time using 50\% of the samples for training with normalized features.}}{9}}
\newlabel{tab.TIME_1_TR}{{VI}{9}}
\newlabel{RF5}{9}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Mean testing time using 50\% of the samples for training without normalized features.}}{9}}
\newlabel{tab.TIME_0_TEST}{{VII}{9}}
\newlabel{RF6}{10}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces Mean testing time using 50\% of the samples for training with normalized features.}}{10}}
\newlabel{tab.TIME_1_TEST}{{VIII}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Discussion}{10}}
\newlabel{ss.discussion}{{\unhbox \voidb@x \hbox {IV-C}}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusions and Future Works}{10}}
\newlabel{s.conclusions}{{V}{10}}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{CortesML:95}{1}
\bibcite{HungNeuro:93}{2}
\bibcite{AdeliMLN:94}{3}
\bibcite{LinTSMCS:15a}{4}
\bibcite{LinTSMCS:15b}{5}
\bibcite{LiuTSMCS:16}{6}
\bibcite{AdeliAMC:94}{7}
\bibcite{AdeliJSA:93}{8}
\bibcite{ChouCACIE:15}{9}
\bibcite{LeCunNature:15}{10}
\bibcite{RafieiCEM:16}{11}
\bibcite{PapaISVC:08}{12}
\bibcite{PapaIJIST:09}{13}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces Recognition rates concerning the medium-to-large datasets.}}{11}}
\newlabel{tab.L_ACC}{{IX}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces Training time concerning the medium-to-large datasets.}}{11}}
\newlabel{tab.L_TR_TIME}{{X}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces Testing time concerning the medium-to-large datasets.}}{11}}
\newlabel{tab.L_TST_TIME}{{XI}{11}}
\@writefile{toc}{\contentsline {section}{References}{11}}
\bibcite{PapaPR:12}{14}
\bibcite{PapaPRL:17}{15}
\bibcite{RumelhartNature:86}{16}
\bibcite{HaganIEEETNN:94}{17}
\bibcite{LiuIEEETSMC:11}{18}
\bibcite{Lin:15}{19}
\bibcite{Martinel:15}{20}
\bibcite{Specht:90}{21}
\bibcite{AhmadlouICAE:10}{22}
\bibcite{AdeliNN:09}{23}
\bibcite{SankariJNM:11}{24}
\bibcite{Sankari:11}{25}
\bibcite{Hirschauer:15}{26}
\bibcite{Zienkiewicz:67}{27}
\bibcite{YuJSE:93}{28}
\bibcite{Lehtinen:08}{29}
\bibcite{Pereira:10}{30}
\bibcite{Shepard:68}{31}
\bibcite{Samworth:12}{32}
\bibcite{Coppersmith:90}{33}
\bibcite{LibOPF:14}{34}
\bibcite{Wilcoxon:45}{35}
